{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kpwaWyzo1Yr"
   },
   "source": [
    "\n",
    "<img src=\"https://i.imgur.com/wVsVeTp.png\" width=800>\n",
    "\n",
    "* 이미지 출처 : [Blei2011-5](https://www.eecis.udel.edu/~shatkay/Course/papers/UIntrotoTopicModelsBlei2011-5.pdf)\n",
    "\n",
    "\n",
    "## 토픽 모델링\n",
    "\n",
    "* 토픽 모델(Topic model)이란 문서 집합의 추상적인 \"주제\"를 발견하기 위한 통계적 모델 중 하나로, 텍스트 본문의 숨겨진 의미구조를 발견하기 위해 사용되는 텍스트 마이닝 기법 중 하나이다. 특정 주제에 관한 문헌에서는 그 주제에 관한 단어가 다른 단어들에 비해 더 자주 등장할 것이다. 예를 들어 개에 대한 문서에서는 \"개\"와 \"뼈다귀\"라는 단어가 더 자주 등장하는 반면, 고양이에 대한 문서에서는 \"고양이\"와 \"야옹\"이 더 자주 등장할 것이고, \"그\", \"~이다\"와 같은 단어는 양쪽 모두에서 자주 등장할 것이다. 이렇게 함께 자주 등장하는 단어들은 대게 유사한 의미를 지니게 되는데 이를 잠재적인 \"주제\"로 정의할 수 있다. 즉, \"개\"와 \"뼈다귀\"를 하나의 주제로 묶고, \"고양이\"와 \"야옹\"을 또 다른 주제로 묶는 모형을 구상할 수 있는데 바로 이것이 토픽 모델의 개략적인 개념이다. 실제로 문헌 내에 어떤 주제가 들어있고, 주제 간의 비중이 어떤지는 문헌 집합 내의 단어 통계를 수학적으로 분석함으로써 알아 낼 수 있다.\n",
    "\n",
    "* 토픽 모델은 또한 확률적 토픽 모델이라고도 불리는데, 이는 광범위한 텍스트 본문의 잠재적 의미 구조를 발견하기 위한 통계적 알고리즘을 가리키는 의미로도 쓰인다. 정보화 시대가 도래하면서 매일 생성되는 텍스트는 인간이 직접 처리할 수 있는 양을 크게 넘어서는데, 토픽 모델은 자동적으로 비정형 텍스트의 집합을 이해하기 쉽도록 조직하고 정리하는 데에 쓰일 수 있다. 또한 토픽 모델은 원래 개발된 목적인 텍스트 마이닝 분야 이외에도 유전자 정보, 이미지, 네트워크와 같은 자료에서 유의미한 구조를 발견하는데에도 유용하게 사용되고 있다. 또한 생물정보학과 같은 응용분야에서도 널리 사용되고 있다.\n",
    "\n",
    "### 주요 용어\n",
    "\n",
    "* 토픽(Topic): 토픽은 문서 집합에서 발견할 수 있는 주제나 개념을 의미, 토픽은 일반적으로 관련 단어들의 집합으로 표현\n",
    "* 문서(Document): 텍스트 데이터의 기본 단위, 토픽 모델링은 이러한 문서들이 다양한 토픽의 조합으로 구성되어 있다고 가정\n",
    "* 단어(Word): 문서를 구성하는 기본 요소, 토픽 모델링에서는 특정 단어의 출현 빈도가 해당 토픽과 관련이 있음을 나타냄\n",
    "\n",
    "### 응용 분야\n",
    "\n",
    "* 문서 분류 및 요약: 대규모 문서 집합을 주제별로 분류하고, 주요 내용을 요약\n",
    "* 추천 시스템: 사용자의 관심사나 선호도에 맞는 콘텐츠를 추천\n",
    "* 시장 조사 및 여론 분석: 소셜 미디어 데이터, 고객 리뷰 등에서 주요 트렌드와 고객의 관심사를 분석\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 잠재 의미 분석(LSI)\n",
    "* 1998년 Papadimitriou, Raghavan, Tamaki, Vempala은 잠재 의미 분석(LSI)이라 불리는 모형을 제시하였다. 이 모형은 최초의 토픽 모델로 여겨지는데, 문헌-용어 행렬을 문헌-의미 행렬과 의미-용어 행렬로 분해하는 과정을 통해 잠재 변수인 의미를 발견하고자 했다. \n",
    "\n",
    "### 확률적 잠재 의미 인덱싱 (Probabilistic latent semantic indexing, PLSI)\n",
    "* 이후 1999년에 토마스 호프만은 문헌-용어 행렬에 용어의 출현 빈도를 출현 확률로 대체하는 확률적 잠재 의미 인덱싱 (Probabilistic latent semantic indexing, PLSI) 모형을 제시하였다.\n",
    "\n",
    "### 잠재 디리클레 할당 (LDA)\n",
    "* 자연어 처리에서 잠재 디리클레 할당(Latent Dirichlet allocation, LDA)은 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지를 서술하는 대한 확률적 토픽 모델 기법 중 하나이다. 미리 알고 있는 주제별 단어수 분포를 바탕으로, 주어진 문서에서 발견된 단어수 분포를 분석함으로써 해당 문서가 어떤 주제들을 함께 다루고 있을지를 예측할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "* [토픽 모델 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/%ED%86%A0%ED%94%BD_%EB%AA%A8%EB%8D%B8)\n",
    "* [잠재 디리클레 할당 - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/%EC%9E%A0%EC%9E%AC_%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88_%ED%95%A0%EB%8B%B9)\n",
    "\n",
    "<img src=\"https://i.imgur.com/NYHk1hg.png\" width=600>\n",
    "\n",
    "* 이미지 출처 : \n",
    "    * https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n",
    "    * https://jmlr.csail.mit.edu/papers/v3/blei03a.html\n",
    "    * [vrjkmr/arxiv-topic: Detecting topic clusters in arXiv ML papers.](https://github.com/vrjkmr/arxiv-topic)\n",
    "\n",
    "\n",
    "## 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4168,
     "status": "ok",
     "timestamp": 1695347885466,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "gWlEwF3FmwiW",
    "outputId": "83faf0bd-da35-4b2b-d495-5e437d5e5d8c"
   },
   "outputs": [],
   "source": [
    "# !pip install koreanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1695347886825,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "7jEmmKL0o1Ys"
   },
   "outputs": [],
   "source": [
    "# 필요 라이브러리를 로드합니다.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MzjjX3ho1Ys"
   },
   "source": [
    "## 데이터 로드\n",
    "* [KLUE Benchmark](https://klue-benchmark.com/)\n",
    "* https://github.com/KLUE-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/KLUE-benchmark/KLUE/main/klue_benchmark/ynat-v1.1/ynat-v1.1_train.json\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1768,
     "status": "ok",
     "timestamp": 1695348492900,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "Qdo8RVgomwiX",
    "outputId": "7c9e15da-9bfd-4ba7-96d3-23ad5e2bb5c9"
   },
   "outputs": [],
   "source": [
    "# read_json 으로 url 데이터 불러오기\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1695348492901,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "W3fN4JkkmwiX",
    "outputId": "81c87938-dc34-4f68-c319-eedaaf31fcc9"
   },
   "outputs": [],
   "source": [
    "# head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52A6h91gmwiX"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1695348507802,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "WCG39r_jmwiX",
    "outputId": "dd8a46cf-7b8f-4654-b784-8ff4ddd25751"
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "# max_features=10000\n",
    "# tfidfvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1695348510374,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "SpSS7OK4mwiY",
    "outputId": "269880fb-8a28-4682-acbe-1dda34d1da31"
   },
   "outputs": [],
   "source": [
    "# df_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXCQLJKGo1Y3"
   },
   "source": [
    "## LDA(LatentDirichletAllocation) 토픽모델링\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 4192,
     "status": "ok",
     "timestamp": 1695348515366,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "w3b8NzMBmwiY",
    "outputId": "6e163551-a819-4bf3-8e41-cf41534993a5"
   },
   "outputs": [],
   "source": [
    "# W는 원본 데이터 X의 각 행이 어떻게 H의 각 행(또는 특성)의 조합으로 표현될 수 있는지를 나타냅니다.\n",
    "# H는 n_components에 지정된 수의 행을 가지며, 원본 데이터 X의 열과 동일한 수의 열을 가집니다.\n",
    "# H는 원본 데이터 X의 열(특성)을 새로운 축소된 차원의 특성으로 표현한 것입니다.\n",
    "# LatentDirichletAllocation\n",
    "# LDA_model\n",
    "# W\n",
    "# H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* W는 일반적으로 \"Weight matrix\" 또는 \"Basis matrix\"라고 불리는 행렬을 나타냅니다. 이 행렬은 원본 데이터를 잠재적 특성(토픽)으로 변환하는 기저를 형성합니다.\n",
    "* H는 \"Coefficient matrix\" 또는 \"Encoding matrix\"라고 불리는 행렬을 나타냅니다. 이 행렬은 잠재적 특성들이 원본 데이터를 어떻게 재구성하는지를 나타내는 계수들을 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임으로 결과 반환하기\n",
    "df_lda_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 토픽의 상위 키워드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주제별로 상위 키워드를 막대그래프로 시각화합니다.\n",
    "def plot_top_words(model, feature_names, n_top_words, title, n_topics=5):    \n",
    "    # df_lda_topic\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_topics, figsize=(6 * n_topics, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_idx in df_lda_topic.index:\n",
    "        ax = axes[topic_idx]\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f\"Topic {topic_idx + 1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "        for i in \"top right\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "        fig.suptitle(title, fontsize=30)\n",
    "        plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "executionInfo": {
     "elapsed": 1273,
     "status": "ok",
     "timestamp": 1695348516636,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "rFrpXSQimwiY",
    "outputId": "c386fbad-aa99-4314-b806-d1df5bc71686"
   },
   "outputs": [],
   "source": [
    "n_top_words = 20\n",
    "\n",
    "plot_top_words(\n",
    "    LDA_model, tfidfvect.get_feature_names_out(), n_top_words, \"Topics in LDA model (LatentDirichletAllocation)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF(Non-Negative Matrix Factorization) 토픽모델링\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn-decomposition-nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28359,
     "status": "ok",
     "timestamp": 1695348544993,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "Pxacv_bMmwiY",
    "outputId": "81c0c895-75dc-4800-c87d-7311609a6cd3"
   },
   "outputs": [],
   "source": [
    "# W는 원본 데이터 X의 각 행이 어떻게 H의 각 행(또는 특성)의 조합으로 표현될 수 있는지를 나타냅니다.\n",
    "# H는 n_components에 지정된 수의 행을 가지며, 원본 데이터 X의 열과 동일한 수의 열을 가집니다.\n",
    "# H는 원본 데이터 X의 열(특성)을 새로운 축소된 차원의 특성으로 표현한 것입니다.\n",
    "# nmf_model\n",
    "n_components = 5\n",
    "n_top_words = 20\n",
    "\n",
    "# Fit the NMF model\n",
    "# nmf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "executionInfo": {
     "elapsed": 1890,
     "status": "ok",
     "timestamp": 1695348546879,
     "user": {
      "displayName": "Joeun Park",
      "userId": "17163383815352082166"
     },
     "user_tz": -540
    },
    "id": "nlg0zSLFmwiZ",
    "outputId": "342672e2-107b-489e-b32b-0b55dcb96ea1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_top_words(\n",
    "    nmf_model, tfidfvect.get_feature_names_out(), n_top_words, \"Topics in NMF model (Frobenius norm)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽별 워드클라우드\n",
    "\n",
    "* https://github.com/amueller/word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud 설치\n",
    "# !pip install wordcloud\n",
    "# !conda install -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공식문서의 튜토리얼을 보고 wordcloud를 그리는 함수를 만들어 봅니다.\n",
    "# 이때 폰트 설정시 폰트명이 아닌 폰트의 설치 경로를 입력해 주셔야 합니다.\n",
    "# 윈도우 : r\"C:\\Windows\\Fonts\\malgun.ttf\" 해당 경로에 폰트가 있는지 확인을 해주세요.\n",
    "# 맥 : r\"/Library/Fonts/AppleGothic.ttf\"\n",
    "# 나눔고딕 등의 폰트를 설치했다면 : '/Library/Fonts/NanumBarunGothic.ttf'\n",
    "\n",
    "# !apt -qq -y install fonts-nanum\n",
    "\n",
    "import platform\n",
    "\n",
    "# 운영체제에 따른 한글 폰트 경로 설정\n",
    "if platform.system() == 'Windows':\n",
    "    # Windows 운영체제의 경우\n",
    "    font_path = r'C:\\Windows\\Fonts\\malgun.ttf'\n",
    "elif platform.system() == 'Darwin':\n",
    "    # macOS의 경우\n",
    "    font_path = r'/Library/Fonts/AppleGothic.ttf'\n",
    "    font_path = r'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "else:\n",
    "    # Linux 또는 기타 운영체제의 경우 (예: 'NanumBarunGothic')\n",
    "    font_path = r'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afeliFmnmwiZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 워드클라우드 생성하는 함수\n",
    "def draw_wordclouds(topic_words, topic_idx, font_path=font_path):\n",
    "      \n",
    "    # 워드클라우드 불러오기\n",
    "    wordcloud = WordCloud(width=800, height=800, \n",
    "                          background_color='white',\n",
    "                          font_path=font_path,\n",
    "                         ).generate_from_frequencies(topic_words)\n",
    "\n",
    "    # 워드클라우드 시각화\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Topic {topic_idx+1}', fontsize=20)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cvect_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_cvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_cvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nmf_h_cvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for topic_idx in range(n_components):\n",
    "#     draw_wordclouds(df_nmf_h_cvect.loc[topic_idx].to_dict(), topic_idx=topic_idx)\n",
    "#     df_nmf_h_cvect.loc[topic_idx].nlargest(20).plot(kind=\"bar\", figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Gf96ryVUeKp9an732gDokUT7RzE9fZNq",
     "timestamp": 1695347852821
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.998px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
