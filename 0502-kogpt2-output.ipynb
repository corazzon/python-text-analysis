{"cells":[{"cell_type":"markdown","source":["* 인프런 강의의 수업자료 입니다\n","* 인프런 강의 주소 : https://inf.run/FX4TP\n","\n"],"metadata":{"id":"IEsSoz-jdF-9"},"id":"IEsSoz-jdF-9"},{"cell_type":"markdown","source":["## Hugging Face의 transformers 라이브러리를 사용하여 한국어 GPT-2 모델을 활용하여 문장 생성하기"],"metadata":{"id":"NEBs8kDyron2"},"id":"NEBs8kDyron2"},{"cell_type":"code","execution_count":1,"id":"61b02b9e","metadata":{"id":"61b02b9e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703402843075,"user_tz":-540,"elapsed":27950,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"dec69438-3d40-4cd8-aa44-95c7ef0e3022"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# transformers는 Hugging Face에서 제공하는 자연어처리 라이브러리로, 다양한 사전학습 언어모델을 제공하고 있습니다.\n","# accelerate는 Hugging Face에서 제공하는 학습 가속 라이브러리로, PyTorch와 함께 사용할 수 있습니다.\n","# 이를 이용하면 단일 노드의 다중 GPU 학습을 지원하고, 학습시간을 단축할 수 있습니다.\n","!pip install transformers --upgrade --q\n","!pip install accelerate --q"]},{"cell_type":"markdown","id":"Z2KLQ8VhSrD0","metadata":{"id":"Z2KLQ8VhSrD0"},"source":["## koGPT2\n","* [skt/kogpt2-base-v2 · Hugging Face](https://huggingface.co/skt/kogpt2-base-v2)\n","\n","* tokenizers 패키지의 Character BPE tokenizer로 학습되었습니다.\n","\n","\n","* 사전 크기는 51,200 이며 대화에 자주 쓰이는 아래와 같은 이모티콘, 이모지 등을 추가하여 해당 토큰의 인식 능력을 올렸습니다.\n","* 또한 <unused0> ~ <unused99>등의 미사용 토큰을 정의해 필요한 테스크에 따라 자유롭게 정의해 사용할 수 있게 했습니다.\n","\n","* BPE(Byte Pair Encoding)은 하나의 텍스트를 토큰으로 분리하기 위한 subword 기반 알고리즘이며, 유사한 알고리즘으로는 WordPiece, SentencePiece 등이 있습니다. 이들 알고리즘은 모두 텍스트를 일정 크기의 subword 단위로 쪼개서 처리하는 과정에서 OOV(Out-Of-Vocabulary) 문제를 해결하고자 개발된 알고리즘입니다.\n","    * Wordpiece 모델에서 Out-of-vocabulary(OOV) 문제를 해결하기 위해, 모든 단어를 서브워드(subword) 단위로 분할하여 처리합니다. 즉, 모든 단어를 이루는 subword들의 조합으로 단어를 대체합니다. 예를 들어 \"playing\"이라는 단어를 Wordpiece 모델에 넣으면 \"play\"와 \"##ing\"으로 분할하여 처리합니다. \"##\"은 서브워드를 나타내는 특별한 기호입니다. 이렇게 단어를 subword들의 집합으로 분할하여 처리함으로써, 이전에 OOV 문제로 처리할 수 없었던 단어를 분할하여 처리할 수 있게 되어 모델의 성능을 향상시킬 수 있습니다.\n","    * SentencePiece는 Google에서 개발한 오픈소스 기반의 자연어 처리 도구입니다. BPE(Byte Pair Encoding)와 Unigram Language Model Tokenizer(ULM)를 적용할 수 있으며, 모델의 입력으로 사용할 수 있는 단어 집합을 만들기 위해 사용됩니다.\n","    * SentencePiece는 형태소 분석 및 품사 태깅을 수행하지 않고, 문자열을 작은 서브워드(subword)로 분할합니다. 이를 통해, OOV(Out of Vocabulary) 문제를 완화하고, 모델의 성능을 높일 수 있습니다. 또한, SentencePiece는 대규모 텍스트 데이터에서 자동으로 언어 모델을 학습하므로, 사전에 훈련된 모델을 사용하여 적은 데이터로도 효과적인 자연어 처리를 수행할 수 있습니다.\n","    * SentencePiece는 PyTorch, TensorFlow, C++, Java, Python 등에서 사용할 수 있습니다. 또한, 다양한 언어(한국어, 영어, 일본어 등)를 지원하며, 사용자가 직접 모델을 학습할 수 있도록 API를 제공합니다.\n","\n","\n","\n","## PreTrainedTokenizerFast\n","\n","* tokenizer를 생성합니다. from_pretrained 함수를 사용하여 사전 학습된 한국어 GPT-2 모델을 불러옵니다. 이때, 시작 토큰, 종료 토큰, 알 수 없는 토큰, 패딩 토큰, 마스크 토큰 등을 설정할 수 있습니다.\n","\n","* 마스크 토큰은 자연어 처리에서 문장 내 일부 단어를 마스킹하여 모델이 이를 예측하는 작업입니다. 마스킹 작업은 모델이 문장 내에서 해당 단어가 빠진 문맥을 이해하고, 문맥에 맞는 적절한 단어를 예측하는 능력을 강화시키는 데에 유용합니다.\n","\n","* 마스크 토큰은 대부분의 transformer 모델에서 사용되며, BERT, RoBERTa, GPT 등에서 사용됩니다. 대표적으로 BERT에서는 15%의 단어를 마스킹하며, 이 중 80%는 [MASK] 토큰으로, 10%는 랜덤한 단어로, 10%는 원래 단어로 대체됩니다.\n","\n","* 한국어 GPT-2 모델에서도 마스크 토큰을 지원하며, tokenizer에서 mask_token 옵션을 사용하여 마스크 토큰을 설정할 수 있습니다. 마스크 토큰을 사용하는 것은 모델의 학습 능력을 향상시키는 데에 큰 도움을 줄 수 있습니다.\n","\n","\n","* bos_token : 토크나이저에서 사용되는 특수 토큰 중 하나로, 문장의 시작을 나타내는 토큰입니다. BOS는 \"Beginning of Sentence\"의 약자로, 이 토큰이 삽입된 위치가 문장의 시작임을 나타냅니다. BOS 토큰은 토크나이징된 시퀀스의 맨 앞에 추가됩니다. 예를 들어, \"Hello, world!\"라는 문장을 BERT 토크나이저로 토큰화하면 \"[CLS] Hello, world! [SEP]\"와 같이 BOS 토큰이 \"[CLS]\"로 추가되어 있습니다.\n","* eos_token: 문자열의 끝을 나타내는 토큰입니다.\n","* unk_token: 알 수 없는 토큰입니다. 모델이 학습하지 않은 단어를 나타낼 때 사용됩니다.\n","* pad_token: 시퀀스 패딩에 사용되는 토큰입니다. 시퀀스의 길이를 일정하게 맞추기 위해 사용됩니다.\n","* mask_token: 마스킹 토큰입니다. 모델의 입력 시퀀스 중 일부 토큰을 마스킹하여 학습에 사용됩니다.\n","\n","* [Tokenizer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)\n","* [skt/kogpt2-base-v2 · Hugging Face](https://huggingface.co/skt/kogpt2-base-v2)"]},{"cell_type":"code","execution_count":3,"id":"yIr2kcIFTG3h","metadata":{"id":"yIr2kcIFTG3h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703403805867,"user_tz":-540,"elapsed":722,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"0535c729-c16e-4f1a-b7c9-074dc76dd4b3"},"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]},{"output_type":"execute_result","data":{"text/plain":["51200"]},"metadata":{},"execution_count":3}],"source":["# PreTrainedTokenizerFast\n","from transformers import PreTrainedTokenizerFast\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","                                                    bos_token='</s>',\n","                                                    eos_token='</d>',\n","                                                    unk_token='<unk>',\n","                                                    pad_token='<pad>',\n","                                                    mask_token='<mask>')\n","len(tokenizer)"]},{"cell_type":"code","execution_count":6,"id":"7934453a","metadata":{"id":"7934453a","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1703403909278,"user_tz":-540,"elapsed":400,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"16653c06-b05b-4e99-e664-e11eab984164"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'안녕하세요. 한국어 GPT-2 입니다.😤 감사합니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["raw_text = \"안녕하세요. 한국어 GPT-2 입니다.😤 감사합니다.\"\n","raw_text"]},{"cell_type":"code","execution_count":8,"id":"c3f8fcea","metadata":{"id":"c3f8fcea","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703403922519,"user_tz":-540,"elapsed":5,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"8aa4a4d6-07bb-40d4-d3e5-233727a8b264"},"outputs":[{"output_type":"stream","name":"stdout","text":["['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', '▁감사', '합니다.']\n"]}],"source":["# tokenize(): 문장을 token으로 나눠서 반환\n","tokens = tokenizer.tokenize(raw_text)\n","print(tokens)"]},{"cell_type":"code","execution_count":10,"id":"b5defa86","metadata":{"id":"b5defa86","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404020035,"user_tz":-540,"elapsed":4,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"6f743cad-71e9-4ccd-8c23-5e88ccdcffd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[25906, 8702, 7801, 25856, 34407, 10528, 422, 426, 18258, 9241, 12521, 254, 15940, 37194]\n"]}],"source":["# convert_tokens_to_ids(): token 목록을 받아서 token_id 목록을 반환\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(token_ids)"]},{"cell_type":"code","execution_count":13,"id":"3f75c0d7","metadata":{"id":"3f75c0d7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404081527,"user_tz":-540,"elapsed":270,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"4bdc3c38-f3c2-4148-c853-0a5428008e18"},"outputs":[{"output_type":"stream","name":"stdout","text":["['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', '▁감사', '합니다.']\n"]}],"source":["# convert_ids_to_tokens(): token_id 목록을 받아서 token 목록을 반환\n","converted_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n","print(converted_tokens)"]},{"cell_type":"code","execution_count":14,"id":"55b9e552","metadata":{"id":"55b9e552","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1703404151116,"user_tz":-540,"elapsed":11,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"54b4ef85-abb1-4dd5-bc67-2efd43758253"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'안녕하세요. 한국어 GPT-2 입니다.😤 감사합니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["# convert_tokens_to_string(): token 목록을 받아서 문장으로 반환\n","converted_text = tokenizer.convert_tokens_to_string(converted_tokens)\n","converted_text"]},{"cell_type":"code","execution_count":17,"id":"5ad92eed","metadata":{"id":"5ad92eed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404195220,"user_tz":-540,"elapsed":4,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"7a56463b-cab4-4f10-8a7f-eea6d82d7c11"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[25906,\n"," 8702,\n"," 7801,\n"," 25856,\n"," 34407,\n"," 10528,\n"," 422,\n"," 426,\n"," 18258,\n"," 9241,\n"," 12521,\n"," 254,\n"," 15940,\n"," 37194]"]},"metadata":{},"execution_count":17}],"source":["# encode(): token으로 자르기 + token을 token_id로 변환하여 반환\n","# = tokenize() + convert_tokens_to_ids()\n","tokenizer.encode(raw_text)"]},{"cell_type":"code","execution_count":18,"id":"8149a623","metadata":{"id":"8149a623","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404210706,"user_tz":-540,"elapsed":8,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"a4f59dbc-a590-4288-cda2-b8d1bf83cee5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('</s>', '</d>', '<unk>', '<pad>', '<mask>')"]},"metadata":{},"execution_count":18}],"source":["# tokenizer가 갖고 있는 특별한 token들을 확인\n","tokenizer.bos_token, tokenizer.eos_token, tokenizer.unk_token, tokenizer.pad_token, tokenizer.mask_token"]},{"cell_type":"code","execution_count":19,"id":"e5b56a26","metadata":{"id":"e5b56a26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404224731,"user_tz":-540,"elapsed":323,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"88580e99-5303-4951-acb0-4d96eb004aa9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 8, 5, 3, 6)"]},"metadata":{},"execution_count":19}],"source":["# tokenizer가 갖고 있는 특별한 token들의 token_id를 확인\n","tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.unk_token_id, tokenizer.pad_token_id, tokenizer.mask_token_id"]},{"cell_type":"code","execution_count":20,"id":"a517cf9e","metadata":{"id":"a517cf9e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404236920,"user_tz":-540,"elapsed":446,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"ed07bafe-3c5c-4e06-a533-ec633069e962"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['▁', '<unk>']"]},"metadata":{},"execution_count":20}],"source":["# UNK: 학습하지 못한 입력을 대체\n","tokenizer.tokenize(\"뷁\")"]},{"cell_type":"markdown","source":["## 다른 문장 토큰화 적용"],"metadata":{"id":"jYM_MFslsn0G"},"id":"jYM_MFslsn0G"},{"cell_type":"code","source":["sample_text = \"\"\"\n","    마스킹은 모델 학습에서 매우 유용한 기술 중 하나입니다.\n","    이를 통해 모델은 입력 시퀀스의 일부를 무작위로 마스킹하여 누락된 토큰을 예측하도록 학습할 수 있습니다.\n","    이렇게 하면 모델은 입력 시퀀스에서 누락된 정보를 추론하는 능력을 향상시키며,\n","    더 일반화된 학습이 가능해집니다.\n","    예를 들어, 기계 번역에서는 마스킹을 사용하여 모델이 문장의 일부 단어를 예측하도록 학습시킬 수 있습니다.\n","    이를 통해 모델은 문장의 구조와 문맥을 이해하고, 번역의 정확도를 향상시킬 수 있습니다.\n","    또한, 마스킹은 모델이 오버피팅(overfitting)을 피하는 데도 도움이 됩니다.\n","    오버피팅은 모델이 학습 데이터에 지나치게 적합하여 새로운 데이터에 대한 일반화 능력이 부족한 상태를 의미합니다.\n","    마스킹을 통해 모델은 입력 시퀀스의 랜덤한 부분을 누락시킴으로써 일반화 능력을 향상시키고, 오버피팅을 방지할 수 있습니다.\n","    따라서, 마스킹은 모델 학습에서 중요한 역할을 합니다. 이를 통해 모델은 누락된 정보를 추론하는 능력을 강화하고,\n","    오버피팅을 방지하며, 더 일반화된 학습이 가능해집니다.\n","    \"\"\"\n","sample_text"],"metadata":{"id":"9kWmJm-8mdyg","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1703404384357,"user_tz":-540,"elapsed":411,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"9e8417be-7186-48ae-9936-8d0e78bb1e70"},"id":"9kWmJm-8mdyg","execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n    마스킹은 모델 학습에서 매우 유용한 기술 중 하나입니다.\\n    이를 통해 모델은 입력 시퀀스의 일부를 무작위로 마스킹하여 누락된 토큰을 예측하도록 학습할 수 있습니다.\\n    이렇게 하면 모델은 입력 시퀀스에서 누락된 정보를 추론하는 능력을 향상시키며,\\n    더 일반화된 학습이 가능해집니다.\\n    예를 들어, 기계 번역에서는 마스킹을 사용하여 모델이 문장의 일부 단어를 예측하도록 학습시킬 수 있습니다.\\n    이를 통해 모델은 문장의 구조와 문맥을 이해하고, 번역의 정확도를 향상시킬 수 있습니다.\\n    또한, 마스킹은 모델이 오버피팅(overfitting)을 피하는 데도 도움이 됩니다.\\n    오버피팅은 모델이 학습 데이터에 지나치게 적합하여 새로운 데이터에 대한 일반화 능력이 부족한 상태를 의미합니다.\\n    마스킹을 통해 모델은 입력 시퀀스의 랜덤한 부분을 누락시킴으로써 일반화 능력을 향상시키고, 오버피팅을 방지할 수 있습니다.\\n    따라서, 마스킹은 모델 학습에서 중요한 역할을 합니다. 이를 통해 모델은 누락된 정보를 추론하는 능력을 강화하고,\\n    오버피팅을 방지하며, 더 일반화된 학습이 가능해집니다.\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","execution_count":24,"id":"oQ4i5XJwTRku","metadata":{"id":"oQ4i5XJwTRku","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404308223,"user_tz":-540,"elapsed":5,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"044896ba-bbb2-4bf3-e8b0-a84863fddd3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['▁\\n', '▁', '▁', '▁', '▁마', '스', '킹', '은', '▁모델', '▁학습', '에서', '▁매우', '▁유용한', '▁기술', '▁중', '▁하나', '입니다.\\n', '▁', '▁', '▁', '▁이를', '▁통해', '▁모델', '은', '▁입력', '▁시', '퀀', '스의', '▁일부를', '▁무작', '위로', '▁마', '스', '킹', '하여', '▁누락', '된', '▁토', '큰', '을', '▁예측', '하도록', '▁학습', '할', '▁수', '▁있습니다.\\n', '▁', '▁', '▁', '▁이렇게', '▁하면', '▁모델', '은', '▁입력', '▁시', '퀀', '스에서', '▁누락', '된', '▁정보를', '▁추론', '하는', '▁능력을', '▁향상', '시키며,', '\\n', '▁', '▁', '▁', '▁더', '▁일반', '화된', '▁학습', '이', '▁가능', '해', '집니다.\\n', '▁', '▁', '▁', '▁예를', '▁들어,', '▁기계', '▁번역', '에서는', '▁마', '스', '킹', '을', '▁사용하여', '▁모델이', '▁문장의', '▁일부', '▁단어를', '▁예측', '하도록', '▁학습', '시킬', '▁수', '▁있습니다.\\n', '▁', '▁', '▁', '▁이를', '▁통해', '▁모델', '은', '▁문장의', '▁구조와', '▁문', '맥을', '▁이해', '하고,', '▁번역', '의', '▁정확', '도를', '▁향상', '시킬', '▁수', '▁있습니다.\\n', '▁', '▁', '▁', '▁또한,', '▁마', '스', '킹', '은', '▁모델이', '▁오버', '피', '팅', '(', 'over', 'f', 'itt', 'ing', ')을', '▁피하는', '▁데도', '▁도움이', '▁됩니다.\\n', '▁', '▁', '▁', '▁오버', '피', '팅', '은', '▁모델이', '▁학습', '▁데이터', '에', '▁지나치게', '▁적합', '하여', '▁새로운', '▁데이터', '에', '▁대한', '▁일반화', '▁능력이', '▁부족한', '▁상태를', '▁의미', '합니다.\\n', '▁', '▁', '▁', '▁마', '스', '킹', '을', '▁통해', '▁모델', '은', '▁입력', '▁시', '퀀', '스의', '▁랜', '덤', '한', '▁부분을', '▁누락', '시킴으로써', '▁일반화', '▁능력을', '▁향상', '시키고,', '▁오버', '피', '팅', '을', '▁방지', '할', '▁수', '▁있습니다.\\n', '▁', '▁', '▁', '▁따라서,', '▁마', '스', '킹', '은', '▁모델', '▁학습', '에서', '▁중요한', '▁역할을', '▁합니다.', '▁이를', '▁통해', '▁모델', '은', '▁누락', '된', '▁정보를', '▁추론', '하는', '▁능력을', '▁강화', '하고,', '\\n', '▁', '▁', '▁', '▁오버', '피', '팅', '을', '▁방지', '하며,', '▁더', '▁일반', '화된', '▁학습', '이', '▁가능', '해', '집니다.\\n', '▁', '▁', '▁', '▁']\n"]}],"source":["# tokenizer.tokenize\n","tokenizer_text = tokenizer.tokenize(sample_text)\n","print(tokenizer_text)"]},{"cell_type":"code","source":["# tokenizer.encode\n","tokenizer.encode(sample_text)"],"metadata":{"id":"KuEJauyGmU_p"},"id":"KuEJauyGmU_p","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.convert_tokens_to_string(tokenizer_text)"],"metadata":{"id":"qUTniwZ-m1_n","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1703404346620,"user_tz":-540,"elapsed":263,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"a80c8e41-2123-4030-b7f1-7599b99c1789"},"id":"qUTniwZ-m1_n","execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n    마스킹은 모델 학습에서 매우 유용한 기술 중 하나입니다.\\n    이를 통해 모델은 입력 시퀀스의 일부를 무작위로 마스킹하여 누락된 토큰을 예측하도록 학습할 수 있습니다.\\n    이렇게 하면 모델은 입력 시퀀스에서 누락된 정보를 추론하는 능력을 향상시키며,\\n    더 일반화된 학습이 가능해집니다.\\n    예를 들어, 기계 번역에서는 마스킹을 사용하여 모델이 문장의 일부 단어를 예측하도록 학습시킬 수 있습니다.\\n    이를 통해 모델은 문장의 구조와 문맥을 이해하고, 번역의 정확도를 향상시킬 수 있습니다.\\n    또한, 마스킹은 모델이 오버피팅(overfitting)을 피하는 데도 도움이 됩니다.\\n    오버피팅은 모델이 학습 데이터에 지나치게 적합하여 새로운 데이터에 대한 일반화 능력이 부족한 상태를 의미합니다.\\n    마스킹을 통해 모델은 입력 시퀀스의 랜덤한 부분을 누락시킴으로써 일반화 능력을 향상시키고, 오버피팅을 방지할 수 있습니다.\\n    따라서, 마스킹은 모델 학습에서 중요한 역할을 합니다. 이를 통해 모델은 누락된 정보를 추론하는 능력을 강화하고,\\n    오버피팅을 방지하며, 더 일반화된 학습이 가능해집니다.\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","id":"jCaKjzxJMqDC","metadata":{"id":"jCaKjzxJMqDC"},"source":["## GPT2LMHeadModel\n","\n"," GPT2LMHeadModel 클래스를 사용하여 모델을 불러옵니다. from_pretrained 함수를 사용하여 사전 학습된 한국어 GPT-2 모델을 불러옵니다.\n","\n","GPT2LMHeadModel는 OpenAI에서 개발한 언어 모델인 GPT-2(GPT-2 Language Model)을 파이토치(PyTorch)로 구현한 모델입니다.\n","\n","GPT-2는 Transformer 아키텍처를 기반으로 한 언어 모델로, 입력된 문장에 대해 다음 단어를 예측하는 과정에서 이전 단어들의 상호작용을 반영합니다. Transformer는 주로 자연어처리 분야에서 사용되는 딥러닝 모델 중 하나로, 이전의 RNN(Recurrent Neural Network) 모델에 비해 계산 속도가 빠르며, 병렬 처리가 용이하여 학습 시간이 단축됩니다.\n","\n","GPT2LMHeadModel은 Transformer 모델의 블록인 GPT2Block을 12개 연결한 transformer와, 언어 모델의 출력층인 lm_head로 구성됩니다. transformer는 입력된 텍스트 시퀀스를 이용해 텍스트를 임베딩하고, 자기어텐션(self-attention)과 전방향(feedforward) 신경망으로 구성된 블록인 GPT2Block을 통해 시퀀스의 정보를 추출합니다. lm_head는 transformer의 출력을 입력받아 다음 단어를 예측하는 과정에서 사용됩니다.\n","\n","따라서 GPT2LMHeadModel는 입력된 텍스트를 이용해 다음 단어를 예측하는 언어 모델입니다. 이를 통해 대화 시스템, 번역, 요약 등의 다양한 자연어 처리 태스크를 수행할 수 있습니다.\n","\n","* [skt/kogpt2-base-v2 · Hugging Face](https://huggingface.co/skt/kogpt2-base-v2)\n","* [OpenAI GPT2](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/gpt2#transformers.GPT2LMHeadModel)"]},{"cell_type":"code","execution_count":29,"id":"e2bc624f","metadata":{"id":"e2bc624f","colab":{"base_uri":"https://localhost:8080/","height":171,"referenced_widgets":["71f50fcc34804396852c8088ef2fd4af","96af4a61fe3c42bb8c7ff0d3dc923736","45624f3b132d4d36ae0d5505cbd97d0b","b71d6600823f481bb1f022252d317638","2cbe28b8d490411dadbb5d8d8543643b","aa119d4a05bd48d385ef712ffb1b4a4e","9584c2f57d5647bc88fbd7bc90fdbab4","00e00dd8c4054226b4705ca3b9537ccb","e82326062d424a2cbb04eeb865f49a66","fe6ccbd0c5704394b3d3bfd3d0837e3f","893cb794a30e49a58ad8568b6a5eb380"]},"executionInfo":{"status":"ok","timestamp":1703404690775,"user_tz":-540,"elapsed":15750,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"3cf4fad1-fd80-4202-cdda-e5766cb76483"},"outputs":[{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f50fcc34804396852c8088ef2fd4af"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\n","특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\n","또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\n","아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\n","운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\n","운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\n","운동을\n"]}],"source":["# GPT2LMHeadModel.from_pretrained\n","# skt/kogpt2-base-v2\n","import torch\n","from transformers import GPT2LMHeadModel\n","\n","model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","text = '근육이 커지기 위해서는'\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","gen_ids = model.generate(input_ids,\n","                           max_length=128,\n","                           repetition_penalty=2.0,\n","                           pad_token_id=tokenizer.pad_token_id,\n","                           eos_token_id=tokenizer.eos_token_id,\n","                           bos_token_id=tokenizer.bos_token_id,\n","                           use_cache=True)\n","generated = tokenizer.decode(gen_ids[0])\n","print(generated)"]},{"cell_type":"markdown","id":"Ea__CDDTOgtE","metadata":{"id":"Ea__CDDTOgtE"},"source":["\n","### GPT2Model 사용하기\n","\n","* GPT2LMHeadModel은 GPT-2 모델의 언어 모델링 버전으로, 입력 시퀀스에 대해 다음 단어를 예측하는 작업을 수행합니다.\n","* 이 모델은 GPT2Model과 LM Head로 구성됩니다.\n","* GPT2Model은 GPT-2 아키텍처를 구현한 모듈이며, 입력 시퀀스를 받아 transformer 레이어를 거치며 출력값을 내보냅니다. 이 모델은 다음과 같은 하위 모듈들로 구성됩니다.\n","* WTE(Word Token Embedding)는 각 단어를 고정 크기의 벡터로 변환하는 기술로, 단어를 벡터 공간에 임베딩하여 자연어 처리 모델에 입력으로 제공합니다. 이를 통해 단어 간 유사도를 측정하거나, 텍스트 데이터의 차원을 줄이는 등 다양한 자연어 처리 작업에서 사용됩니다. WTE는 일반적으로 원-핫 인코딩 방법과 달리, 단어의 의미를 나타내는 잠재적인 구조를 학습하도록 설계됩니다. 이를 위해 WTE는 주로 언어 모델에 사용되는 신경망 아키텍처인 임베딩 레이어와 결합하여 사용됩니다.\n","\n","    * wte: 입력 토큰의 임베딩 테이블입니다.\n","    * wpe: 입력 토큰의 위치 임베딩 테이블입니다.\n","    * drop: 입력 토큰의 임베딩과 위치 임베딩에 대한 드롭아웃을 수행합니다.\n","    * h: GPT-2 블록들의 스택으로 구성된 transformer 레이어입니다.\n","    * ln_f: 출력값에 대한 Layer Normalization을 수행합니다.\n","\n","\n","트랜스포머 모델에서 Layer Normalization은 다음과 같은 수식으로 정의됩니다.\n","\n","$$\\text{LayerNorm}(x_i) = \\gamma \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n","\n","\n","트랜스포머 모델에서 Layer Normalization은 각 레이어의 입력을 정규화하는 기법 중 하나입니다. 각 레이어의 입력은 해당 레이어에서 연산이 수행되기 전에 정규화됩니다. 이러한 정규화는 미니배치 내에서 각 샘플의 평균과 분산을 사용하여 수행됩니다. 즉, 레이어별로 입력에 대한 평균과 분산을 구하고, 이를 사용하여 입력을 정규화합니다.\n","\n","Layer Normalization은 Batch Normalization과 비슷한 목적을 가지지만, Batch Normalization은 미니배치 단위로 입력을 정규화하는 반면, Layer Normalization은 레이어 내에서 입력을 정규화합니다. 이러한 차이점으로 인해 Layer Normalization은 Batch Normalization과 비교해 작은 미니배치나 긴 시퀀스의 경우에도 안정적으로 학습할 수 있는 장점이 있습니다.\n","\n","Layer Normalization은 RNN과 같이 시퀀스 모델링에서 성능 향상에 효과적인 것으로 알려져 있습니다. 이는 시퀀스 모델링에서 배치 정규화를 적용하기 어려운 점과 관련이 있습니다. 배치 정규화는 미니배치 크기에 따라 성능이 크게 달라질 수 있는 반면, Layer Normalization은 샘플 단위로 계산하기 때문에 미니배치 크기에 영향을 받지 않습니다. 또한 Layer Normalization은 모델의 불안정성을 줄이고 학습 속도를 빠르게 하는 효과도 있습니다.\n","\n","\n","여기서 $x$는 입력 텐서, $\\mu$와 $\\sigma^2$는 평균과 분산으로, 미니배치 내에서 샘플별로 계산됩니다. $\\gamma$와 $\\beta$는 학습 가능한 스케일(scale)과 시프트(shift) 매개변수입니다. $\\epsilon$은 작은 값으로 분모가 0이 되는 것을 방지합니다.\n","\n","\n","\n","* $x_i$ : $i$번째 입력 특성 값\n","* $\\mu$ : 평균\n","* $\\sigma$ : 표준편차\n","* $\\epsilon$ : 작은 값으로 분모가 0이 되는 것을 방지하기 위한 값\n","* $\\gamma$ : 스케일 조정 파라미터\n","* $\\beta$ : 이동 조정 파라미터\n","* $\\text{LN}(x_i)$ : 정규화된 $i$번째 입력 특성 값\n","\n","\n","\n","\n","### GPT-2 블록\n","\n","은 self-attention과 MLP 레이어로 구성되어 있습니다. 이 중 self-attention은 입력 시퀀스 내에서 다양한 위치들 간의 상호작용을 수행하여, 특정 위치의 출력값이 전체 시퀀스의 컨텍스트에 따라 동적으로 결정되는 효과를 가집니다.\n","\n","#### LM Head\n","\n","LM Head는 transformer 레이어의 출력값을 받아, 다음 단어 예측을 수행하는 선형 레이어입니다. 이 레이어는 단어의 분포를 출력값으로 내보내며, 이 분포에서 가장 높은 확률을 가지는 단어가 모델의 예측값이 됩니다.\n","\n","\n","* https://jalammar.github.io/illustrated-transformer/\n","\n","<img src=\"https://nlpinkorean.github.io/images/transformer/transformer_resideual_layer_norm_3.png\">"]},{"cell_type":"code","execution_count":30,"id":"87ZeMrfVMa0A","metadata":{"id":"87ZeMrfVMa0A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703404754085,"user_tz":-540,"elapsed":4,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"53cca835-2af3-414c-fee5-6fab91f196b5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(51200, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",")"]},"metadata":{},"execution_count":30}],"source":["# model\n","model"]},{"cell_type":"markdown","id":"otEfAP3jVAv5","metadata":{"id":"otEfAP3jVAv5"},"source":["* 참고 :\n","* Conv1D와 Dot Product 모두 벡터 간의 곱셈을 수행합니다. 그러나 Conv1D는 한 쪽 벡터를 이동시키면서 다른 쪽 벡터와의 내적을 계산하는 반면, Dot Product는 두 벡터의 요소 간 곱의 합으로 구성됩니다.\n","\n","* Conv1D는 커널(kernel)을 이동시키면서 입력 벡터와의 내적을 계산합니다. 이 때 커널의 크기는 고정되어 있지만, 이동하는 위치에 따라 결과가 달라질 수 있습니다. 이를 통해 입력 벡터 내의 패턴을 인식하거나 특징(feature)을 추출하는 등의 역할을 수행할 수 있습니다.\n","\n","* 반면, Dot Product는 두 벡터의 요소 간 곱의 합으로 구성됩니다. 이를 통해 두 벡터 간의 유사도를 측정하거나, 각 요소가 독립적인 경우에 대해 연산을 수행할 수 있습니다.\n","\n","* 따라서 Conv1D와 Dot Product는 비슷한 연산이지만, Conv1D는 입력 벡터 내에서 패턴을 인식하는 데에 적합하고, Dot Product는 벡터 간의 유사도 측정 등에 적합합니다."]},{"cell_type":"markdown","id":"a6K2J5wQR-EZ","metadata":{"id":"a6K2J5wQR-EZ"},"source":["## 생성\n","generate 함수를 사용하여 모델을 이용하여 문장을 생성합니다. input_ids에 입력으로 넣을 텍스트를 tokenize한 결과를 Tensor 형태로 입력합니다. max_length는 최대 생성 문장 길이, repetition_penalty는 반복되는 단어를 억제하는 페널티 값입니다. pad_token_id, eos_token_id, bos_token_id는 tokenizer에서 설정한 값과 동일하게 입력해주어야 합니다.\n","\n","마지막으로, decode 함수를 사용하여 생성된 Tensor 값을 텍스트로 디코딩합니다."]},{"cell_type":"markdown","id":"UT3pr-qG0pHv","metadata":{"id":"UT3pr-qG0pHv"},"source":["* tokenizer.encode(text, return_tensors='pt'): 주어진 text를 tokenizer를 활용하여 토큰화하고, 이를 pytorch tensor 형태로 변환합니다.\n","* model.generate(): pytorch tensor 형태로 변환된 input_ids를 GPT2 모델에 입력으로 넣어 자연어 생성을 수행합니다. max_length는 생성할 문장의 최대 길이를 의미하며, repetition_penalty는 반복을 방지하는데 사용되는 계수입니다. pad_token_id, eos_token_id, bos_token_id는 각각 패딩, 문장 끝, 문장 시작을 의미하는 토큰의 id를 나타냅니다. use_cache는 이전 단계의 계산 결과를 재사용할지 여부를 나타냅니다.\n","* tokenizer.decode(gen_ids[0]): GPT2 모델의 출력값인 gen_ids를 tokenizer를 활용하여 토큰을 텍스트로 디코딩합니다.\n"]},{"cell_type":"code","execution_count":31,"id":"nZPLIoOsR8t-","metadata":{"id":"nZPLIoOsR8t-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703405216428,"user_tz":-540,"elapsed":14424,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"8f005438-855d-4eae-82fe-b834079b6318"},"outputs":[{"output_type":"stream","name":"stdout","text":["꾸준한 운동은 괜찮지만 운동량이 부족하면 오히려 관절에 무리를 줄 수 있다.\n","관절염으로 인한 통증은 주로 무릎이나 허리, 엉덩이 등 관절이 많이 아픈 부위에 나타난다.\n","이런 경우라면 평소 운동을 꾸준히 하는 것이 좋다.\n","특히, 걷기 운동이 가장 좋은데, 특히나 계단 오르내리기, 자전거 타기와 같은 유산소운동을 할 때 통증이 심하다면 전문의를 찾아 정확한 진단을 받는 게 중요하다.\n","또한 등산 시에도 무리하지 말고 가벼운 스트레칭을 통해 근육과 인대를 강화시키는 것도 도움이 된다.\n","운동 후 바로 샤워를 하거나 가볍게 몸을 풀어주는 것만으로도 통증을 줄일 수는 있지만, 너무 오래 방치할 경우에는 부\n"]}],"source":["text = '꾸준한 운동은 '\n","\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","gen_ids = model.generate(input_ids,\n","                           max_length=128,\n","                           repetition_penalty=2.0,\n","                           pad_token_id=tokenizer.pad_token_id,\n","                           eos_token_id=tokenizer.eos_token_id,\n","                           bos_token_id=tokenizer.bos_token_id,\n","                           use_cache=True)\n","generated = tokenizer.decode(gen_ids[0])\n","print(generated)"]},{"cell_type":"code","execution_count":44,"id":"a13a3090","metadata":{"id":"a13a3090","executionInfo":{"status":"ok","timestamp":1703405388268,"user_tz":-540,"elapsed":351,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}}},"outputs":[],"source":["# text를 받아 생성한 문장을 반환하는 함수\n","def generate_text(text):\n","    input_ids = tokenizer.encode(text, return_tensors='pt')\n","    gen_ids = model.generate(input_ids,\n","                            max_length=512,\n","                            repetition_penalty=5.0,\n","                            pad_token_id=tokenizer.pad_token_id,\n","                            eos_token_id=tokenizer.eos_token_id,\n","                            bos_token_id=tokenizer.bos_token_id,\n","                            use_cache=True)\n","    generated = tokenizer.decode(gen_ids[0])\n","    return generated"]},{"cell_type":"code","execution_count":45,"id":"b7113ba1","metadata":{"id":"b7113ba1","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1703405388638,"user_tz":-540,"elapsed":10,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"373dd4d8-bfba-4d06-de06-0c1267ca49d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'봄 여행지 추천?</d>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}],"source":["text = '봄 여행지 추천?'\n","generate_text(text)"]},{"cell_type":"code","execution_count":46,"id":"34053e99","metadata":{"id":"34053e99","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1703405405869,"user_tz":-540,"elapsed":16880,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"b46d1fa1-3722-4f91-ce75-3ece5216f2eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'오늘의 점심 식사는?\"\\n\"아니야. 그게 아니라면.\"\\n그녀는 고개를 끄덕였다.\\n그리고는 다시 한 번 말했다.\\n\\'그래, 내가 너를 사랑하고 있다는 걸 알았어!\\'\\n이제 그녀는 자신의 마음을 다잡았다.\\n하지만 그녀의 마음은 여전히 불안했다.\\n왜냐하면 그녀가 자신을 사랑하는 것은 그녀 자신이기 때문이다.\\n그러나 그녀를 향한 그의 사랑은 결코 끝나지 않았다.\\n그는 그녀에게 사랑을 고백하지 않을 수 없었다.\\n그러자 그는 더 이상 그를 미워할 수가 없게 되었다.\\n결국 그가 그에게로 돌아가지 않기로 결심했다.\\n그의 마음속에 있는 모든 것을 빼앗아버리고 싶었다.\\n더 이상은 견딜 수도 없어.\\n나는 이제 그만 돌아가고 싶어!\\n내가 원하는 건 오직 하나,\\n나의 사랑이 아니란 말이지?\\n내 안에 있던 것들을 모두 빼앗아 버릴 거야, 라고 생각했다.\\n지금까지 해왔던 것처럼 말이다.\\n다시 말해서, 나는 지금 이 순간부터 내 안의 어떤 것까지도 전부 잃어버리게 될 것이다.\\n모든 것이 사라져 버린다는 사실을 깨달을 때까지만이라도 당신을 용서해 주겠다.\\n당신이 나를 위해 무엇을 할 것인가?\\n나를 위해서 무엇이든 해야 한다.\\n나에게 필요한 것, 그리고 나 자신에게 필요했던 것들이 무엇인지 알고 있다.\\n또한 당신의 삶을 바꿀 수는 없다.\\n다음 날 아침은 너무나도 행복하게 느껴졌다.\\n마지막으로 우리는 서로를 향해 손을 흔들며 인사를 나누었다.\\n우리는 서로의 얼굴을 마주보며 이야기를 나누었고, 서로에 대한 그리움을 달랬다.</d>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}],"source":["text = '오늘의 점심 식사는?'\n","generate_text(text)"]},{"cell_type":"code","execution_count":47,"id":"HHLnOLlNLire","metadata":{"id":"HHLnOLlNLire","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1703405415462,"user_tz":-540,"elapsed":9597,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"7fe8ea2c-80f9-4da2-e43e-3f6519f86bda"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'샐러드 레시피를 공개했다.\\n이날 방송에서 김 대표는 “요즘은 다이어트 식품으로 많이 찾는 것이 사실”이라며 “이런 식품을 먹으면서 칼로리가 높은 음식을 먹는 것은 좋지 않다”고 말했다.\\n이어 그는 “아무리 좋은 음식이라도 몸에 나쁜 영향을 끼치면 안 된다”면서 “그렇기 때문에 평소에는 잘 먹지 않는다. 특히 과식을 하지 않도록 주의해야 한다”고 덧붙였다.\\n김 대표가 소개한 ‘다크초콜릿’은 초콜릿의 단맛을 내는 폴리페놀 성분이 들어있어 체내 흡수율을 높여준다.\\n또한 비타민C가 풍부해 피로회복에 도움을 준다.\\n특히, 다크로쉐에 함유된 안토시아닌 성분은 혈중 콜레스테롤 수치를 낮춰주는 효과가 있다.\\n아울러 항산화 물질인 베타카로틴과 칼슘도 풍부하다.\\n이에 따라 당뇨병 환자나 고혈압 환자 등 만성질환을 가진 사람은 물론 비만이나 고지혈증 환자에게 좋다.\\n최근엔 식이섬유와 섬유질이 풍부한 건강기능식품들이 인기를 끌고 있다.</d>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}],"source":["text = '샐러드 레시피'\n","generate_text(text)"]},{"cell_type":"code","execution_count":48,"id":"L6BbjNadLv4d","metadata":{"id":"L6BbjNadLv4d","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1703405417123,"user_tz":-540,"elapsed":1666,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}},"outputId":"aedff728-a73f-4535-e714-d648ff9ef1e2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"파이썬 코딩 공부 방법 추천\\n1. 딥러닝을 활용한 학습법 소개를 통해 자신의 실력을 점검해보자.\\n2. '스마트폰으로 촬영한 이미지'를 활용해 자신만의 이미지를 만들어보자.</d>\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}],"source":["generate_text(\"파이썬 코딩 공부 방법 추천\")"]},{"cell_type":"code","execution_count":48,"id":"auRcQ2uHMOia","metadata":{"id":"auRcQ2uHMOia","executionInfo":{"status":"ok","timestamp":1703405417124,"user_tz":-540,"elapsed":8,"user":{"displayName":"오늘코드todaycode","userId":"06313463015165663090"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"71f50fcc34804396852c8088ef2fd4af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96af4a61fe3c42bb8c7ff0d3dc923736","IPY_MODEL_45624f3b132d4d36ae0d5505cbd97d0b","IPY_MODEL_b71d6600823f481bb1f022252d317638"],"layout":"IPY_MODEL_2cbe28b8d490411dadbb5d8d8543643b"}},"96af4a61fe3c42bb8c7ff0d3dc923736":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa119d4a05bd48d385ef712ffb1b4a4e","placeholder":"​","style":"IPY_MODEL_9584c2f57d5647bc88fbd7bc90fdbab4","value":"pytorch_model.bin: 100%"}},"45624f3b132d4d36ae0d5505cbd97d0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00e00dd8c4054226b4705ca3b9537ccb","max":513302779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e82326062d424a2cbb04eeb865f49a66","value":513302779}},"b71d6600823f481bb1f022252d317638":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe6ccbd0c5704394b3d3bfd3d0837e3f","placeholder":"​","style":"IPY_MODEL_893cb794a30e49a58ad8568b6a5eb380","value":" 513M/513M [00:04&lt;00:00, 123MB/s]"}},"2cbe28b8d490411dadbb5d8d8543643b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa119d4a05bd48d385ef712ffb1b4a4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9584c2f57d5647bc88fbd7bc90fdbab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00e00dd8c4054226b4705ca3b9537ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e82326062d424a2cbb04eeb865f49a66":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe6ccbd0c5704394b3d3bfd3d0837e3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"893cb794a30e49a58ad8568b6a5eb380":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}