{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy6Elih7gVZ_"
   },
   "source": [
    "## 단어 벡터화 하기\n",
    "\n",
    "<img src=\"https://i.imgur.com/hii5VdV.png\" width=\"600\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy6Elih7gVZ_"
   },
   "source": [
    "* https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "## 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBIC6ap4XNO6"
   },
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFlB8K5kIhC9"
   },
   "outputs": [],
   "source": [
    "# 데이터 분석을 위한 pandas, 수치계산을 위한 numpy, 시각화를 위한 seaborn, matplotlib 을 로드합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화를 위한 한글폰트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29oJws6LgzIR"
   },
   "outputs": [],
   "source": [
    "def get_font_family():\n",
    "    \"\"\"\n",
    "    시스템 환경에 따른 기본 폰트명을 반환하는 함수\n",
    "    \"\"\"\n",
    "    import platform\n",
    "    system_name = platform.system()\n",
    "\n",
    "    if system_name == \"Darwin\" :\n",
    "        font_family = \"AppleGothic\"\n",
    "    elif system_name == \"Windows\":\n",
    "        font_family = \"Malgun Gothic\"\n",
    "    else:\n",
    "        # Linux(Colab)\n",
    "        !apt-get install fonts-nanum -qq  > /dev/null\n",
    "        !fc-cache -fv\n",
    "\n",
    "        import matplotlib as mpl\n",
    "        mpl.font_manager._rebuild()\n",
    "        findfont = mpl.font_manager.fontManager.findfont\n",
    "        mpl.font_manager.findfont = findfont\n",
    "        mpl.backends.backend_agg.findfont = findfont\n",
    "        \n",
    "        font_family = \"NanumBarunGothic\"\n",
    "    return font_family\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.rc(\"font\", family=get_font_family())\n",
    "plt.rc(\"axes\", unicode_minus=False)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhy1ATcYrSgi"
   },
   "source": [
    "## 분석할 문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv-6jXPabA2E"
   },
   "outputs": [],
   "source": [
    "corpus = [\"코로나 거리두기와 코로나 상생지원금 문의입니다.\",\n",
    "          \"지하철 운행시간과 지하철 요금 문의입니다.\",\n",
    "          \"지하철 승강장 문의입니다.\",\n",
    "          \"코로나 선별진료소 문의입니다.\",\n",
    "          \"버스 운행시간 문의입니다.\", \n",
    "          \"버스 터미널 위치 안내입니다.\",\n",
    "          \"코로나 거리두기 안내입니다.\",\n",
    "          \"택시 승강장 문의입니다.\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5c3Qt8lwoP6"
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "\n",
    "* BOW(bag of words)\n",
    "    * 가장 간단하지만 효과적이라 널리쓰이는 방법\n",
    "    * 장, 문단, 문장, 서식과 같은 입력 텍스트의 구조를 제외하고 각 단어가 이 말뭉치에 얼마나 많이 나타나는지만 헤아립니다.\n",
    "    * 구조와 상관없이 단어의 출현횟수만 세기 때문에 텍스트를 담는 가방(bag)으로 생각할 수 있습니다.\n",
    "    * BOW는 단어의 순서가 완전히 무시 된다는 단점이 있다. 예를 들어 의미가 완전히 반대인 두 문장이 있다고 합니다.\n",
    "        - `it's bad, not good at all.` \n",
    "        - `it's good, not bad at all.` \n",
    "    * 위 두 문장은 의미가 전혀 반대지만 완전히 동일하게 반환됩니다.\n",
    "    * 이를 보완하기 위해 n-gram을 사용하는 데 BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용할 수 있도록 합니다.\n",
    "    * min_df는 문서에 특정 단어가 최소 몇 번 이상 문서에 등장하는 단어를 가방에 담겠다는 의미입니다.\n",
    "\n",
    "* [Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "\n",
    "### CountVectorizer\n",
    "* CountVectorizer 는 사이킷런에서 제공하는 bag of words 를 만들 수 있는 방법입니다.\n",
    "* 텍스트 문서 모음을 토큰 수의 행렬로 변환합니다.\n",
    "\n",
    "* 단어들의 카운트(출현 빈도)로 여러 문서들을 벡터화\n",
    "* 문서목록에서 각 문서의 feature(문장의 특징) 노출수를 가중치로 설정한 BOW 벡터를 생성\n",
    "* 카운트 행렬, 단어 문서 행렬 (Term-Document Matrix, TDM))\n",
    "* max_df, min_df 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있음 \n",
    "* 토큰의 빈도가 max_df로 지정한 값을 초과 하거나 min_df로 지정한 값보다 작은 경우에는 무시하며 인자 값은 정수인 경우 횟수, 부동소수점인 경우 비율을 의미\n",
    "* API documentation:https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "### 사용법\n",
    "1. 문서를 토큰 리스트로 변환한다.\n",
    "2. 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "3. 각 문서를 BOW 인코딩 벡터로 변환한다.\n",
    "4. 매개 변수\n",
    "    * analyzer : 단어, 문자 단위의 벡터화 방법 정의\n",
    "    * ngram_range : BOW 단위 수 (1, 3) 이라면 1개~3개까지 토큰을 묶어서 벡터화\n",
    "    * max_df : 어휘를 작성할 때 문서 빈도가 주어진 임계값보다 높은 용어(말뭉치 관련 불용어)는 제외 (기본값=1.0)\n",
    "        * max_df = 0.90 : 문서의 90% 이상에 나타나는 단어 제외\n",
    "        * max_df = 10 : 10개 이상의 문서에 나타나는 단어 제외\n",
    "    * min_df : 어휘를 작성할 때 문서 빈도가 주어진 임계값보다 낮은 용어는 제외합니다. 컷오프라고도 합니다.(기본값=1.0)\n",
    "        * min_df = 0.01 : 문서의 1% 미만으로 나타나는 단어 제외\n",
    "        * min_df = 10 : 문서에 10개 미만으로 나타나는 단어 제외\n",
    "    * stop_words : 불용어 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwxFNuC2NET4"
   },
   "outputs": [],
   "source": [
    "# sklearn.feature_extraction.text의 CountVectorizer 를 통해 BOW 를 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdXYRQqgQS2V"
   },
   "source": [
    "### fit, transform, fit_transfrom의 차이점\n",
    "* fit(): 원시 문서에 있는 모든 토큰의 어휘 사전을 배웁니다.\n",
    "* transform(): 문서를 문서 용어 매트릭스로 변환합니다. transform 이후엔 매트릭스로 변환되어 숫자형태로 변경됩니다.\n",
    "* fit_transform(): 어휘 사전을 배우고 문서 용어 매트릭스를 반환합니다. fit 다음에 변환이 오는 것과 동일하지만 더 효율적으로 구현됩니다.\n",
    "\n",
    "⚠️ 주의! ⚠️\n",
    "* 단, fit_transform 은 학습데이터에만 사용하고 예측 데이터에는 transform 을 사용합니다.\n",
    "* 예측 데이터에도 fit_transform 을 사용하게 된다면 서로 다른 단어사전으로 행렬을 만들게 됩니다.\n",
    "* fit 과 transform 을 따로 사용해 준다 하더라도 fit 은 학습 데이터에만 사용합니다. 같은 단어 사전으로 예측 데이터셋의 단어 사전을 만들기 위해서 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kj92S-EBF-yB"
   },
   "outputs": [],
   "source": [
    "# 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 Document Term Matrix(이하 dtm) 을 반환합니다.\n",
    "# fit(): 원시 문서에 있는 모든 토큰의 어휘 사전을 배웁니다.\n",
    "# transform(): 문서를 단어 빈도수가 들어있는 문서 용어 매트릭스로 변환합니다. \n",
    "# cvect\n",
    "# dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform() 어휘 사전을 배우고 문서 용어 매트릭스를 반환합니다. \n",
    "# fit 다음에 변환이 오는 것과 동일하지만 더 효율적으로 구현됩니다.\n",
    "# dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_YEjZbIE4L1"
   },
   "outputs": [],
   "source": [
    "# get_feature_names is deprecated in 1.0 and will be removed in 1.2\n",
    "# 1.2 버전에서 제외될 예정으로 get_feature_names_out()을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_feature_names_out 로 단어 사전 보기\n",
    "# vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXSatDPzF-vt"
   },
   "outputs": [],
   "source": [
    "# 단어사전을 봅니다. \n",
    "# 단어사전은 {\"단어\": 인덱스번호} 로 되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document-term matrix 를 데이터프레임으로 만듭니다.\n",
    "# df_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문서에서 단어 빈도의 합계를 구합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus 와 document-term matrix 를 비교해 봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIPr3DaCPohL"
   },
   "source": [
    "### N-grams \n",
    "\n",
    "* 토큰을 몇 개 사용할 것인지를 구분합니다. 지정한 n개의 숫자 만큼의 토큰을 묶어서 사용합니다.\n",
    "* 예를 들어 (1, 1) 이라면 1개의 토큰을 (2, 3)이라면 2~3개의 토큰을 사용합니다.\n",
    "* analyzer 설정에 따라 단어단위, 캐릭터 단위에 따라 사용할 수 있습니다.\n",
    "\n",
    "* 기본값 = (1, 1)\n",
    "* ngram_range(min_n, max_n)\n",
    "* min_n <= n <= max_n\n",
    "```\n",
    "(1, 1) 은 1 <= n <= 1\n",
    "(1, 2) 은 1 <= n <= 2\n",
    "(2, 2) 은 2 <= n <= 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어가 너무 많아서 출력이 오래 걸린다면 max_columns 값을 조정해서 사용합니다.\n",
    "# pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15LxBmxkHg8U"
   },
   "outputs": [],
   "source": [
    "# ngram_range: 추출할 다른 단어 n-gram 또는 char n-gram에 대한 n-값 범위의 하한 및 상한입니다. 기본값 = (1, 1)\n",
    "# ngram_range=(1, 2)\n",
    "# cvect\n",
    "# dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dtm.sum 으로 빈도수 합계 구하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 받아 변환을 하고 문서 용어 행렬을 반환하는 함수를 만들어 재사용합니다.\n",
    "def display_transform_dtm(cvect, corpus):\n",
    "    \"\"\"\n",
    "    모델을 받아 변환을 하고 문서 용어 행렬을 반환하는 함수\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_transform_dtm  함수가 잘 만들어졌는지 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwUe4xXfPyYj"
   },
   "outputs": [],
   "source": [
    "# ngram_range=(2, 3)\n",
    "# display_transform_dtm 으로 확인합니다.\n",
    "# cvect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWEivLuaJ5te"
   },
   "source": [
    "### min_df\n",
    "\n",
    "* 기본값=1\n",
    "* min_df는 문서 빈도(문서의 %에 있음)가 지정된 임계값보다 엄격하게 낮은 용어를 무시합니다. \n",
    "* 예를 들어, min_df=0.66은 용어가 어휘의 일부로 간주되려면 문서의 66%에 나타나야 합니다.\n",
    "* 때때로 min_df가 어휘 크기를 제한하는 데 사용됩니다.\n",
    "* 예를들어 min_df를 0.1, 0.2로 설정한다면 10%, 20%에 나타나는 용어만 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLoRKB46J4-6"
   },
   "outputs": [],
   "source": [
    "# min_df=0.1\n",
    "# display_transform_dtm 으로 확인합니다.\n",
    "\n",
    "# cvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdwIi9dOLoHb"
   },
   "source": [
    "### max_df\n",
    "\n",
    "* 기본값=**1**\n",
    "* max_df=int : 빈도수를 의미\n",
    "* max_df=float : 비율을 의미\n",
    "* 어휘를 작성할 때 주어진 임계값보다 문서 빈도가 엄격히 높은 용어는 무시\n",
    "* 빈번하게 등장하는 불용어 등을 제거하기에 좋음\n",
    "* 예를 들어 코로나 관련 기사를 분석하면 90%에 '코로나'라는 용어가 등장할 수 있는데, 이 경우 max_df=0.89 로 비율을 설정하여 너무 빈번하게 등장하는 단어를 제외할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPcoz3dCL8Vx"
   },
   "outputs": [],
   "source": [
    "# max_df=int : 빈도수를 의미\n",
    "# max_df=float : 비율을 의미\n",
    "# display_transform_dtm 으로 확인합니다.\n",
    "\n",
    "# cvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IseYq_4RZk5"
   },
   "source": [
    "### max_features\n",
    "\n",
    "* 기본값 = None\n",
    "* 벡터라이저가 학습할 기능(어휘)의 양 제한\n",
    "* corpus중 빈도수가 가장 높은 순으로 해당 갯수만큼만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ad5WAo1PyVI"
   },
   "outputs": [],
   "source": [
    "# max_features 갯수만큼의 단어만 추출하기\n",
    "# display_transform_dtm 으로 확인합니다.\n",
    "\n",
    "# cvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cp2dE7wYPyS8"
   },
   "outputs": [],
   "source": [
    "# max_features 갯수만큼의 단어만 추출하기\n",
    "# display_transform_dtm 으로 확인합니다.\n",
    "\n",
    "# cvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 stop_words\n",
    "\n",
    "* 문장에 자주 등장하지만 \"우리, 그, 그리고, 그래서\" 등 관사, 전치사, 조사, 접속사 등의 단어로 문장 내에서 큰 의미를 갖지 않는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=[\"코로나\", \"문의입니다\"]\n",
    "# max_features 갯수만큼의 단어만 추출하기\n",
    "\n",
    "# cvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWIgHaWFbYyZ"
   },
   "source": [
    "### analyzer\n",
    "\n",
    "* 기본값='word'\n",
    "* 종류: word, char, char_wb\n",
    "* 기능을 단어 n-그램으로 만들지 문자 n-그램으로 만들어야 하는지 여부입니다. 옵션 'char_wb'는 단어 경계 내부의 텍스트에서만 문자 n-gram을 생성합니다. 단어 가장자리의 n-gram은 공백으로 채워집니다.\n",
    "* 띄어쓰기가 제대로 되어 있지 않은 문자 등에 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YK3L74Esbizy"
   },
   "outputs": [],
   "source": [
    "# analyzer='char_wb\n",
    "\n",
    "# cvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sW3jX8omddHS"
   },
   "outputs": [],
   "source": [
    "# analyzer='char', ngram_range=(2, 3)\n",
    "\n",
    "# cvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJFrsTk0m-kR"
   },
   "source": [
    "## TF-IDF\n",
    "TF(단어 빈도, term frequency)는 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값으로, 이 값이 높을수록 문서에서 중요하다고 생각할 수 있다. 하지만 단어 자체가 문서군 내에서 자주 사용되는 경우, 이것은 그 단어가 흔하게 등장한다는 것을 의미한다. 이것을 DF(문서 빈도, document frequency)라고 하며, 이 값의 역수를 IDF(역문서 빈도, inverse document frequency)라고 한다. TF-IDF는 TF와 IDF를 곱한 값이다.\n",
    "\n",
    "IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어 '원자'라는 낱말은 일반적인 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다.\n",
    "\n",
    "역문서 빈도(IDF)는 한 단어가 문서 집합 전체에서 얼마나 공통적으로 나타나는지를 나타내는 값이다. 전체 문서의 수를 해당 단어를 포함한 문서의 수로 나눈 뒤 로그를 취하여 얻을 수 있다.\n",
    "\n",
    "* 출처 : [TF-IDF - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/TF-IDF)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{tfidf}(w, d) = \\text{tf} \\times (\\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### TfidfVectorizer\n",
    "* 문서 모음을 TF-IDF 기능의 매트릭스로 변환합니다.\n",
    "* tf-idf를 사용하는 목표는 주어진 말뭉치에서 매우 자주 발생하고 따라서 훈련 말뭉치의 작은 부분에서 발생하는 특징보다 경험적으로 덜 유용한 토큰의 영향을 축소하는 것입니다.\n",
    "* API documentation : https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "\n",
    "* TfidfVectorizer 는 사이킷런에서 TF-IDF 가중치를 적용한 단어 벡터를 만들 수 있는 방법입니다.\n",
    "    * norm='l2' 각 문서의 피처 벡터 정규화 방법\n",
    "        - L2 : 벡터의 각 원소의 제곱의 합이 1이 되도록 만드는 것이고 기본 값\n",
    "        - L1 : 벡터의 각 원소의 절댓값의 합이 1이 되도록 크기를 조절\n",
    "    * smooth_idf=False\n",
    "        - 피처를 만들 때 0으로 나오는 항목에 대해 작은 값을 더해서(스무딩을 해서) 피처를 만들지 아니면 그냥 생성할지를 결정\n",
    "    * sublinear_tf=False\n",
    "    * use_idf=True\n",
    "        - TF-IDF를 사용해 피처를 만들 것인지 아니면 단어 빈도 자체를 사용할 것인지 여부\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jyOWdiGTCSR"
   },
   "outputs": [],
   "source": [
    "# sklearn.feature_extraction.text 에서 TfidfVectorizer 를 불러옵니다.\n",
    "# fit, transform 으로 변환을 합니다.\n",
    "# tfidfvect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform 으로 변환할 수도 있습니다.\n",
    "# dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dtm.toarray() 로 배열을 확인합니다.\n",
    "# 문서에 토큰이 더 많이 나타날수록 가중치는 더 커집니다. \n",
    "# 그러나 토큰이 문서에 많이 표시될수록 가중치가 감소합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display_transform_dtm 으로 변환 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNrqEJNYS1GX"
   },
   "source": [
    "### IDF\n",
    "\n",
    "**IDF**\n",
    "\n",
    "- IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어 '원자'라는 낱말은 일반적인 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkaUmQ_rTCMj"
   },
   "outputs": [],
   "source": [
    "# 하나의 문서에만 나타나는 토큰은 idf 가중치가 높습니다.\n",
    "# 적게 나타난 토큰이라도 모든 문서에도 있는 토큰은 idf가 낮습니다.\n",
    "# idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikcpPibOUiSz"
   },
   "outputs": [],
   "source": [
    "# 사전만들기\n",
    "# dict, zip 을 사용하여 피처명과 idf 값을 딕셔너리 형태로 만듭니다.\n",
    "# vocab\n",
    "# idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict 값 시각화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer 의 다양한 기능 사용하기\n",
    "* analyzer\n",
    "* n-gram\n",
    "* min_df, max_df\n",
    "* max_features\n",
    "* stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxrR99jEk714"
   },
   "outputs": [],
   "source": [
    "# analyzer='char_wb', ngram_range=(2, 3), max_df=1.0, min_df=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "KLUE2_sklearn.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
